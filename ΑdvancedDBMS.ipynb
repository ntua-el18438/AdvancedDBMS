{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef8b0fa-5204-4042-af9c-b415f61e7672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>472</td><td>application_1738075734771_0472</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0472/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-220.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0472_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|   Age Group|Total Cases|\n",
      "+------------+-----------+\n",
      "|      Adults|     121052|\n",
      "|Young Adults|      33588|\n",
      "|    Children|      15923|\n",
      "|     Seniors|       5985|\n",
      "+------------+-----------+\n",
      "\n",
      "Execution Time (DataFrame API): 4.208456993103027 seconds"
     ]
    }
   ],
   "source": [
    "#QUERY 1 WITH DF\n",
    "from pyspark.sql.functions import col, when, count, lower\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Δημιουργία SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Φόρτωση δεδομένων από τα δύο αρχεία CSV\n",
    "crime_data_path_2010_2019 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_path_2020_present = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "crime_df_2010_2019 = spark.read.csv(crime_data_path_2010_2019, header=True, inferSchema=True)\n",
    "crime_df_2020_present = spark.read.csv(crime_data_path_2020_present, header=True, inferSchema=True)\n",
    "\n",
    "# Ένωση των δύο DataFrames\n",
    "combined_crime_df = crime_df_2010_2019.union(crime_df_2020_present)\n",
    "\n",
    "# Φιλτράρισμα εγγραφών που αναφέρονται στο Null Island\n",
    "filtered_crime_df = combined_crime_df.filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "\n",
    "# Φιλτράρισμα για περιστατικά που περιέχουν τον όρο \"aggravated assault\"\n",
    "aggr_as_df = filtered_crime_df.filter(lower(col(\"Crm Cd Desc\")).like(\"%aggravated assault%\"))\n",
    "\n",
    "# Προσθήκη στήλης για την κατηγοριοποίηση των θυμάτων σε ηλικιακές ομάδες\n",
    "age_grouped_df = aggr_as_df.withColumn(\n",
    "    \"Age Group\",\n",
    "    when(col(\"Vict Age\") < 18, \"Children\")\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young Adults\")\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"Adults\")\n",
    "    .when(col(\"Vict Age\") > 64, \"Seniors\")\n",
    ")\n",
    "\n",
    "# Ομαδοποίηση ανά ηλικιακή ομάδα και καταμέτρηση\n",
    "result_1_df = age_grouped_df.groupBy(\"Age Group\").agg(count(\"*\").alias(\"Total Cases\"))\n",
    "\n",
    "# Ταξινόμηση σε φθίνουσα σειρά\n",
    "sorted_result_1_df = result_1_df.orderBy(col(\"Total Cases\").desc())\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sorted_result_1_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time (DataFrame API): {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4927ffe5-1af7-4a0f-ac89-a7880e72bf2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time (RDD API): 0.34343934059143066 seconds\n",
      "Adults: 121052\n",
      "Young Adults: 33588\n",
      "Children: 15923\n",
      "Seniors: 5985"
     ]
    }
   ],
   "source": [
    "#QUERY 1 WITH RDD\n",
    "crime_rdd_2010_2019 = spark.read.csv(crime_data_path_2010_2019, header=True, inferSchema=True).rdd\n",
    "crime_rdd_2020_present = spark.read.csv(crime_data_path_2020_present, header=True, inferSchema=True).rdd\n",
    "\n",
    "# Ένωση των δύο RDD\n",
    "combined_rdd = crime_rdd_2010_2019.union(crime_rdd_2020_present)\n",
    "\n",
    "# Φιλτράρισμα για Null Island\n",
    "filtered_rdd = combined_rdd.filter(lambda row: row[\"LAT\"] != 0 and row[\"LON\"] != 0)\n",
    "\n",
    "# Φιλτράρισμα για τον όρο \"aggravated assault\"\n",
    "assault_rdd = filtered_rdd.filter(lambda row: \"aggravated assault\" in row[\"Crm Cd Desc\"].lower())\n",
    "\n",
    "# Κατηγοριοποίηση θυμάτων σε ηλικιακές ομάδες\n",
    "def categorize_age(row):\n",
    "    if row[\"Vict Age\"] < 18:\n",
    "        age_group = \"Children\"\n",
    "    elif 18 <= row[\"Vict Age\"] <= 24:\n",
    "        age_group = \"Young Adults\"\n",
    "    elif 25 <= row[\"Vict Age\"] <= 64:\n",
    "        age_group = \"Adults\"\n",
    "    elif row[\"Vict Age\"] > 64:\n",
    "        age_group = \"Seniors\"\n",
    "    else:\n",
    "        age_group = \"Unknown\"\n",
    "    return (age_group, 1)\n",
    "\n",
    "age_group_rdd = assault_rdd.map(categorize_age)\n",
    "\n",
    "# Συγκέντρωση και υπολογισμός πλήθους ανά ηλικιακή ομάδα\n",
    "age_group_count_rdd = age_group_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Ταξινόμηση σε φθίνουσα σειρά\n",
    "sorted_rdd = age_group_count_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "result = sorted_rdd.collect()  # Χρήση collect() για να πάρουμε τα αποτελέσματα\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time (RDD API): {end_time - start_time} seconds\")\n",
    "\n",
    "# Εκτύπωση αποτελεσμάτων\n",
    "for group, count in result:\n",
    "    print(f\"{group}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70990264-d2eb-49a0-8893-02daf80d794b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+-----------+------------------+----+\n",
      "|YEAR|  AREA NAME|closed_cases|total_cases|  closed_case_rate|rank|\n",
      "+----+-----------+------------+-----------+------------------+----+\n",
      "|2010|    Rampart|        2860|       8706|    32.85090742017|   1|\n",
      "|2010|    Olympic|        2762|       8764|31.515289821999087|   2|\n",
      "|2010|     Harbor|        2818|       9598| 29.36028339237341|   3|\n",
      "|2011|    Olympic|        2798|       7987| 35.03192688118192|   1|\n",
      "|2011|    Rampart|        2744|       8443|32.500296103280824|   2|\n",
      "|2011|     Harbor|        2806|       9840|28.516260162601625|   3|\n",
      "|2012|    Olympic|        2923|       8523|34.295435879385195|   1|\n",
      "|2012|    Rampart|        2791|       8598|32.461037450569904|   2|\n",
      "|2012|     Harbor|        2781|       9416|29.534834324553948|   3|\n",
      "|2013|    Olympic|        2789|       8305| 33.58217940999398|   1|\n",
      "|2013|    Rampart|        2616|       8148|  32.1060382916053|   2|\n",
      "|2013|     Harbor|        2504|       8429| 29.70696405267529|   3|\n",
      "|2014|   Van Nuys|        3031|       9471| 32.00295639320029|   1|\n",
      "|2014|West Valley|        2504|       7946|31.512710797885727|   2|\n",
      "|2014|    Mission|        3113|       9972| 31.21740874448456|   3|\n",
      "|2015|   Van Nuys|        3383|      10485|32.265140677157845|   1|\n",
      "|2015|    Mission|        3245|      10651|30.466622852314334|   2|\n",
      "|2015|   Foothill|        2356|       7762|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|        3382|      10507|  32.1880650994575|   1|\n",
      "|2016|West Valley|        3045|       9696|31.404702970297027|   2|\n",
      "|2016|   Foothill|        2390|       7991|29.908647228131645|   3|\n",
      "|2017|   Van Nuys|        3469|      10824| 32.04915003695491|   1|\n",
      "|2017|    Mission|        3381|      10887|31.055387158996968|   2|\n",
      "|2017|   Foothill|        2504|       8218|30.469700657094183|   3|\n",
      "|2018|   Foothill|        2496|       8122|30.731346958877126|   1|\n",
      "|2018|    Mission|        3136|      10205|30.730034296913278|   2|\n",
      "|2018|   Van Nuys|        3031|      10486|28.905206942590123|   3|\n",
      "|2019|    Mission|        2825|       9196|30.719878207916484|   1|\n",
      "|2019|West Valley|        2668|       8726| 30.57529223011689|   2|\n",
      "|2019|N Hollywood|        3258|      11141|29.243335427699492|   3|\n",
      "|2020|West Valley|        2489|       8080|30.804455445544555|   1|\n",
      "|2020|    Mission|        2557|       8431|30.328549401020044|   2|\n",
      "|2020|     Harbor|        2635|       8848|29.780741410488247|   3|\n",
      "|2021|    Mission|        2497|       8172|30.555555555555557|   1|\n",
      "|2021|West Valley|        2475|       8518| 29.05611645926274|   2|\n",
      "|2021|   Foothill|        1973|       6998| 28.19376964847099|   3|\n",
      "|2022|West Valley|        2729|      10284|26.536367172306498|   1|\n",
      "|2022|     Harbor|        2422|       9196|26.337538060026098|   2|\n",
      "|2022|    Topanga|        2482|       9460|26.236786469344608|   3|\n",
      "|2023|   Foothill|        1914|       7155|26.750524109014673|   1|\n",
      "|2023|    Topanga|        2558|       9639|26.538022616453986|   2|\n",
      "|2023|    Mission|        2304|       8978|25.662731120516817|   3|\n",
      "|2024|N Hollywood|        1279|       6494|19.695103172158916|   1|\n",
      "|2024|   Foothill|         667|       3573|18.667786174083403|   2|\n",
      "|2024|77th Street|        1090|       6188|  17.6147382029735|   3|\n",
      "+----+-----------+------------+-----------+------------------+----+\n",
      "\n",
      "Execution Time (DataFrame API): 3.5827667713165283 seconds"
     ]
    }
   ],
   "source": [
    "#QUERY 2 WITH DF\n",
    "from pyspark.sql.functions import col, count, when, year, to_date, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Μετατροπή \"DATE OCC\" σε ημερομηνία και εξαγωγή του έτους\n",
    "filtered_crime_df = filtered_crime_df.withColumn(\"DATE OCC\", to_date(col(\"DATE OCC\"), \"MM/dd/yyyy\"))\n",
    "filtered_crime_df = filtered_crime_df.withColumn(\"YEAR\", year(col(\"DATE OCC\")))\n",
    "\n",
    "# Υπολογισμός του ποσοστού κλεισμένων υποθέσεων\n",
    "closed_rate_df = filtered_crime_df.groupBy(\"YEAR\", \"AREA NAME\").agg(\n",
    "    count(when(col(\"Status\").isin(\"JA\", \"AA\", \"JO\", \"AO\"), True)).alias(\"closed_cases\"),\n",
    "    count(\"*\").alias(\"total_cases\")\n",
    ").withColumn(\n",
    "    \"closed_case_rate\", (col(\"closed_cases\") / col(\"total_cases\")) * 100\n",
    ")\n",
    "\n",
    "# Δημιουργία παραθύρου για ταξινόμηση\n",
    "window_spec = Window.partitionBy(\"YEAR\").orderBy(col(\"closed_case_rate\").desc())\n",
    "\n",
    "# Υπολογισμός του ranking\n",
    "ranked_df = closed_rate_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Επιλογή των 3 πρώτων τμημάτων ανά έτος\n",
    "top3_df = ranked_df.filter(col(\"rank\") <= 3).orderBy(\"YEAR\", \"rank\")\n",
    "\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Εμφάνιση αποτελεσμάτων\n",
    "top3_df.show(50)\n",
    "\n",
    "end_time_df = time.time()\n",
    "print(f\"Execution Time (DataFrame API): {end_time_df - start_time_df} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8099dbd1-6bdc-4457-bbe5-490187e7677b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+-----------+-----------------+----+\n",
      "|YEAR|  AREA NAME|closed_cases|total_cases| closed_case_rate|rank|\n",
      "+----+-----------+------------+-----------+-----------------+----+\n",
      "|2010|    Rampart|        2860|       8706|32.85090742017000|   1|\n",
      "|2010|    Olympic|        2762|       8764|31.51528982199909|   2|\n",
      "|2010|     Harbor|        2818|       9598|29.36028339237341|   3|\n",
      "|2011|    Olympic|        2798|       7987|35.03192688118192|   1|\n",
      "|2011|    Rampart|        2744|       8443|32.50029610328082|   2|\n",
      "|2011|     Harbor|        2806|       9840|28.51626016260163|   3|\n",
      "|2012|    Olympic|        2923|       8523|34.29543587938519|   1|\n",
      "|2012|    Rampart|        2791|       8598|32.46103745056990|   2|\n",
      "|2012|     Harbor|        2781|       9416|29.53483432455395|   3|\n",
      "|2013|    Olympic|        2789|       8305|33.58217940999398|   1|\n",
      "|2013|    Rampart|        2616|       8148|32.10603829160530|   2|\n",
      "|2013|     Harbor|        2504|       8429|29.70696405267529|   3|\n",
      "|2014|   Van Nuys|        3031|       9471|32.00295639320030|   1|\n",
      "|2014|West Valley|        2504|       7946|31.51271079788573|   2|\n",
      "|2014|    Mission|        3113|       9972|31.21740874448456|   3|\n",
      "|2015|   Van Nuys|        3383|      10485|32.26514067715784|   1|\n",
      "|2015|    Mission|        3245|      10651|30.46662285231434|   2|\n",
      "|2015|   Foothill|        2356|       7762|30.35300180365885|   3|\n",
      "|2016|   Van Nuys|        3382|      10507|32.18806509945750|   1|\n",
      "|2016|West Valley|        3045|       9696|31.40470297029703|   2|\n",
      "|2016|   Foothill|        2390|       7991|29.90864722813165|   3|\n",
      "|2017|   Van Nuys|        3469|      10824|32.04915003695492|   1|\n",
      "|2017|    Mission|        3381|      10887|31.05538715899697|   2|\n",
      "|2017|   Foothill|        2504|       8218|30.46970065709418|   3|\n",
      "|2018|   Foothill|        2496|       8122|30.73134695887712|   1|\n",
      "|2018|    Mission|        3136|      10205|30.73003429691328|   2|\n",
      "|2018|   Van Nuys|        3031|      10486|28.90520694259012|   3|\n",
      "|2019|    Mission|        2825|       9196|30.71987820791649|   1|\n",
      "|2019|West Valley|        2668|       8726|30.57529223011689|   2|\n",
      "|2019|N Hollywood|        3258|      11141|29.24333542769949|   3|\n",
      "|2020|West Valley|        2489|       8080|30.80445544554455|   1|\n",
      "|2020|    Mission|        2557|       8431|30.32854940102005|   2|\n",
      "|2020|     Harbor|        2635|       8848|29.78074141048825|   3|\n",
      "|2021|    Mission|        2497|       8172|30.55555555555556|   1|\n",
      "|2021|West Valley|        2475|       8518|29.05611645926274|   2|\n",
      "|2021|   Foothill|        1973|       6998|28.19376964847099|   3|\n",
      "|2022|West Valley|        2729|      10284|26.53636717230650|   1|\n",
      "|2022|     Harbor|        2422|       9196|26.33753806002610|   2|\n",
      "|2022|    Topanga|        2482|       9460|26.23678646934461|   3|\n",
      "|2023|   Foothill|        1914|       7155|26.75052410901468|   1|\n",
      "|2023|    Topanga|        2558|       9639|26.53802261645399|   2|\n",
      "|2023|    Mission|        2304|       8978|25.66273112051682|   3|\n",
      "|2024|N Hollywood|        1279|       6494|19.69510317215892|   1|\n",
      "|2024|   Foothill|         667|       3573|18.66778617408340|   2|\n",
      "|2024|77th Street|        1090|       6188|17.61473820297350|   3|\n",
      "+----+-----------+------------+-----------+-----------------+----+\n",
      "\n",
      "Execution Time (SQL API): 16.159993648529053 seconds"
     ]
    }
   ],
   "source": [
    "#QUERY 2 WITH SQL API\n",
    "from pyspark.sql.functions import to_date, year\n",
    "\n",
    "# Ρύθμιση legacy time parser\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Μετατροπή της στήλης \"DATE OCC\" σε ημερομηνία και εξαγωγή του έτους\n",
    "filtered_crime_df = filtered_crime_df.withColumn(\"DATE OCC\", to_date(col(\"DATE OCC\"), \"MM/dd/yyyy\"))\n",
    "filtered_crime_df = filtered_crime_df.withColumn(\"YEAR\", year(col(\"DATE OCC\")))\n",
    "\n",
    "# Δημιουργία προσωρινού view για χρήση SQL\n",
    "filtered_crime_df.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# Υλοποίηση του query\n",
    "sql_query = \"\"\"\n",
    "WITH closed_cases_data AS (\n",
    "    SELECT\n",
    "        YEAR,\n",
    "        `AREA NAME`,\n",
    "        COUNT(CASE WHEN Status IN ('JA', 'AA', 'JO', 'AO') THEN 1 END) AS closed_cases,\n",
    "        COUNT(*) AS total_cases,\n",
    "        (COUNT(CASE WHEN Status IN ('JA', 'AA', 'JO', 'AO') THEN 1 END) * 100.0 / COUNT(*)) AS closed_case_rate\n",
    "    FROM crime_data\n",
    "    GROUP BY YEAR, `AREA NAME`\n",
    "),\n",
    "ranked_data AS (\n",
    "    SELECT\n",
    "        YEAR,\n",
    "        `AREA NAME`,\n",
    "        closed_cases,\n",
    "        total_cases,\n",
    "        closed_case_rate,\n",
    "        ROW_NUMBER() OVER (PARTITION BY YEAR ORDER BY closed_case_rate DESC) AS rank\n",
    "    FROM closed_cases_data\n",
    ")\n",
    "SELECT\n",
    "    YEAR,\n",
    "    `AREA NAME`,\n",
    "    closed_cases,\n",
    "    total_cases,\n",
    "    closed_case_rate,\n",
    "    rank\n",
    "FROM ranked_data\n",
    "WHERE rank <= 3\n",
    "ORDER BY YEAR, rank\n",
    "\"\"\"\n",
    "\n",
    "start_time_sql = time.time()\n",
    "\n",
    "# Εκτέλεση του SQL query\n",
    "top3_sql_df = spark.sql(sql_query)\n",
    "top3_sql_df.show(50)\n",
    "\n",
    "end_time_sql = time.time()\n",
    "print(f\"Execution Time (SQL API): {end_time_sql - start_time_sql} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d8e34e8-3f31-4ed3-9393-45053ec60dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------+---------------------+\n",
      "|COMM                   |Crime Per Capita|Mean Estimated Income|\n",
      "+-----------------------+----------------+---------------------+\n",
      "|                       |0.001976        |82810.42857142857    |\n",
      "|Acton                  |0.000127        |78868.25             |\n",
      "|Adams-Normandie        |0.000254        |28084.0              |\n",
      "|Agoura Hills           |0.000049        |114417.0             |\n",
      "|Alhambra               |0.000020        |51822.333333333336   |\n",
      "|Alsace                 |0.000085        |38330.0              |\n",
      "|Altadena               |0.000028        |80547.66666666667    |\n",
      "|Anaverde               |0.001211        |69699.66666666667    |\n",
      "|Angeles National Forest|0.009003        |73013.5294117647     |\n",
      "|Angelino Heights       |0.000638        |39784.5              |\n",
      "|Arcadia                |0.000032        |66056.0              |\n",
      "|Arleta                 |0.000030        |44588.0              |\n",
      "|Athens Village         |0.000200        |33731.0              |\n",
      "|Athens-Westmont        |0.000043        |40645.5              |\n",
      "|Atwater Village        |0.000071        |70560.0              |\n",
      "|Avalon                 |0.000268        |60333.0              |\n",
      "|Azusa                  |0.000017        |66978.25             |\n",
      "|Baldwin Hills          |0.000062        |37447.0              |\n",
      "|Bel Air                |0.000217        |141712.33333333334   |\n",
      "|Bell Gardens           |0.000024        |41109.0              |\n",
      "+-----------------------+----------------+---------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "#QUERY3\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Διαδρομή του αρχείου εισοδήματος\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "\n",
    "# Φόρτωση του αρχείου CSV στο Spark\n",
    "income_df = spark.read.csv(income_path, header=True, inferSchema=True)\n",
    "\n",
    "# Καθαρισμός της στήλης \"Estimated Median Income\"\n",
    "income_df = income_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, sum, broadcast\n",
    "\n",
    "\n",
    "# Εγγραφή Sedona functions\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Υπολογισμός συνολικού πληθυσμού και αριθμού κατοικιών\n",
    "census_population_df = flattened_df.groupBy(\"COMM\", \"ZCTA10\").agg(\n",
    "    sum(\"POP_2010\").alias(\"Total Population\"),\n",
    "    sum(\"HOUSING10\").alias(\"Total Housing Units\"),\n",
    "     ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    ")\n",
    "\n",
    "# Υπολογισμός μέσου μεγέθους νοικοκυριού\n",
    "census_population_df = census_population_df.withColumn(\n",
    "    \"Average Household Size\",\n",
    "    (col(\"Total Population\") / col(\"Total Housing Units\")).cast(\"double\")\n",
    ")\n",
    "\n",
    "# Φιλτράρισμα μηδενικών τιμών (προαιρετικό)\n",
    "census_population_df = census_population_df.filter(\n",
    "    (col(\"Total Population\") > 0) & (col(\"Total Housing Units\") > 0)\n",
    ")\n",
    "\n",
    "# Σύνδεση GeoJSON και εισοδηματικών δεδομένων με βάση το Zip Code\n",
    "joined_income_df = income_df.join(\n",
    "    broadcast(census_population_df),\n",
    "    income_df[\"Zip Code\"] == census_population_df[\"ZCTA10\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Υπολογισμός μέσου εισοδήματος ανά άτομο\n",
    "final_income_df = joined_income_df.withColumn(\n",
    "    \"Mean Income Per Person\",\n",
    "    (col(\"Estimated Median Income\").cast(\"double\") / col(\"Average Household Size\")).cast(\"double\")\n",
    ")\n",
    "\n",
    "# Δημιουργία γεωμετρικών σημείων από LAT και LON\n",
    "crime_points_df = filtered_crime_df.withColumn(\n",
    "    \"geom\", ST_Point(\"LON\", \"LAT\")\n",
    ")\n",
    "\n",
    "# Geospatial join μεταξύ σημείων εγκλημάτων και περιοχών απογραφής\n",
    "crime_population_df = crime_points_df.join(\n",
    "    census_population_df.hint(\"shuffle_hash\"),\n",
    "    ST_Within(crime_points_df[\"geom\"], census_population_df[\"geometry\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Υπολογισμός συνολικών εγκλημάτων και πληθυσμού ανά περιοχή\n",
    "crime_population_df = crime_population_df.groupBy(\"COMM\").agg(\n",
    "    count(\"*\").alias(\"Total Crimes\"),\n",
    "    sum(\"Total Population\").alias(\"Total Population\")\n",
    ").withColumn(\n",
    "    \"Crime Per Capita\",\n",
    "    (col(\"Total Crimes\") / col(\"Total Population\")).cast(\"double\")\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Υπολογισμός μέσου εισοδήματος ανά περιοχή\n",
    "income_aggregated_df = final_income_df.groupBy(\"COMM\").agg(\n",
    "    avg(\"Estimated Median Income\").alias(\"Mean Estimated Income\")\n",
    ")\n",
    "\n",
    "# Σύνδεση του Crime Per Capita με το μέσο εισόδημα\n",
    "final_df = crime_population_df.hint(\"merge\").join(\n",
    "    income_aggregated_df,\n",
    "    \"COMM\",\n",
    "    \"inner\"\n",
    ").select(\n",
    "    \"COMM\",\n",
    "    \"Crime Per Capita\",\n",
    "    \"Mean Estimated Income\"\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "# Μορφοποίηση της στήλης \"Crime Per Capita\" σε δεκαδικούς αριθμούς\n",
    "formatted_df = final_df.withColumn(\n",
    "    \"Crime Per Capita\", format_number(\"Crime Per Capita\", 6)  # Διατήρηση 6 δεκαδικών ψηφίων\n",
    ")\n",
    "\n",
    "# Εμφάνιση αποτελεσμάτων\n",
    "formatted_df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f394507a-1aa6-4173-bd5e-1adb50010b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Join:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#4107, Total Crimes#4602L, Total Population#4604L, Crime Per Capita#4608, Mean Estimated Income#10679]\n",
      "   +- SortMergeJoin [COMM#4107], [COMM#10308], Inner\n",
      "      :- Sort [COMM#4107 ASC NULLS FIRST], false, 0\n",
      "      :  +- Project [COMM#4107, Total Crimes#4602L, Total Population#4604L, (cast(Total Crimes#4602L as double) / cast(Total Population#4604L as double)) AS Crime Per Capita#4608]\n",
      "      :     +- HashAggregate(keys=[COMM#4107], functions=[count(1), sum(Total Population#4313L)], schema specialized)\n",
      "      :        +- Exchange hashpartitioning(COMM#4107, 1000), ENSURE_REQUIREMENTS, [plan_id=4799]\n",
      "      :           +- HashAggregate(keys=[COMM#4107], functions=[partial_count(1), partial_sum(Total Population#4313L)], schema specialized)\n",
      "      :              +- Project [COMM#4107, Total Population#4313L]\n",
      "      :                 +- RangeJoin geom#4462: geometry, geometry#4320: geometry, WITHIN\n",
      "      :                    :- Union\n",
      "      :                    :  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#4462]\n",
      "      :                    :  :  +- Filter ((((isnotnull(LAT#55) AND isnotnull(LON#56)) AND NOT (LAT#55 = 0.0)) AND NOT (LON#56 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                    :  :     +- FileScan csv [LAT#55,LON#56] Batched: false, DataFilters: [isnotnull(LAT#55), isnotnull(LON#56), NOT (LAT#55 = 0.0), NOT (LON#56 = 0.0), isnotnull( **org.a..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "      :                    :  +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#10859]\n",
      "      :                    :     +- Filter ((((isnotnull(LAT#129) AND isnotnull(LON#130)) AND NOT (LAT#129 = 0.0)) AND NOT (LON#130 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                    :        +- FileScan csv [LAT#129,LON#130] Batched: false, DataFilters: [isnotnull(LAT#129), isnotnull(LON#130), NOT (LAT#129 = 0.0), NOT (LON#130 = 0.0), isnotnull( **o..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "      :                    +- Project [COMM#4107, Total Population#4313L, geometry#4320]\n",
      "      :                       +- Filter ((((isnotnull(Total Population#4313L) AND isnotnull(Total Housing Units#4315L)) AND (Total Population#4313L > 0)) AND (Total Housing Units#4315L > 0)) AND isnotnull(geometry#4320))\n",
      "      :                          +- ObjectHashAggregate(keys=[COMM#4107, ZCTA10#4124], functions=[sum(POP_2010#4116L), sum(HOUSING10#4113L), st_union_aggr(geometry#4094, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35967ac4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "      :                             +- Exchange hashpartitioning(COMM#4107, ZCTA10#4124, 1000), ENSURE_REQUIREMENTS, [plan_id=4792]\n",
      "      :                                +- ObjectHashAggregate(keys=[COMM#4107, ZCTA10#4124], functions=[partial_sum(POP_2010#4116L), partial_sum(HOUSING10#4113L), partial_st_union_aggr(geometry#4094, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35967ac4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "      :                                   +- Project [features#4091.properties.COMM AS COMM#4107, features#4091.properties.HOUSING10 AS HOUSING10#4113L, features#4091.properties.POP_2010 AS POP_2010#4116L, features#4091.properties.ZCTA10 AS ZCTA10#4124, features#4091.geometry AS geometry#4094]\n",
      "      :                                      +- Filter isnotnull(features#4091.properties.COMM)\n",
      "      :                                         +- Generate explode(features#4083), false, [features#4091]\n",
      "      :                                            +- Filter ((size(features#4083, true) > 0) AND isnotnull(features#4083))\n",
      "      :                                               +- FileScan geojson [features#4083] Batched: false, DataFilters: [(size(features#4083, true) > 0), isnotnull(features#4083)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- Sort [COMM#10308 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#10308], functions=[avg(Estimated Median Income#10453)], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#10308, 1000), ENSURE_REQUIREMENTS, [plan_id=4765]\n",
      "               +- HashAggregate(keys=[COMM#10308], functions=[partial_avg(Estimated Median Income#10453)], schema specialized)\n",
      "                  +- Project [Estimated Median Income#10453, COMM#10308]\n",
      "                     +- BroadcastHashJoin [Zip Code#10447], [cast(ZCTA10#10325 as int)], Inner, BuildRight, false\n",
      "                        :- Project [Zip Code#10447, cast(regexp_replace(Estimated Median Income#10449, [^0-9.], , 1) as double) AS Estimated Median Income#10453]\n",
      "                        :  +- Filter isnotnull(Zip Code#10447)\n",
      "                        :     +- FileScan csv [Zip Code#10447,Estimated Median Income#10449] Batched: false, DataFilters: [isnotnull(Zip Code#10447)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "                        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[1, string, true] as int) as bigint)),false), [plan_id=4760]\n",
      "                           +- Project [COMM#10308, ZCTA10#10325]\n",
      "                              +- Filter (((isnotnull(Total Population#10514L) AND isnotnull(Total Housing Units#10516L)) AND (Total Population#10514L > 0)) AND (Total Housing Units#10516L > 0))\n",
      "                                 +- HashAggregate(keys=[COMM#10308, ZCTA10#10325], functions=[sum(POP_2010#10317L), sum(HOUSING10#10314L)], schema specialized)\n",
      "                                    +- Exchange hashpartitioning(COMM#10308, ZCTA10#10325, 1000), ENSURE_REQUIREMENTS, [plan_id=4755]\n",
      "                                       +- HashAggregate(keys=[COMM#10308, ZCTA10#10325], functions=[partial_sum(POP_2010#10317L), partial_sum(HOUSING10#10314L)], schema specialized)\n",
      "                                          +- Project [features#10292.properties.COMM AS COMM#10308, features#10292.properties.HOUSING10 AS HOUSING10#10314L, features#10292.properties.POP_2010 AS POP_2010#10317L, features#10292.properties.ZCTA10 AS ZCTA10#10325]\n",
      "                                             +- Filter (isnotnull(features#10292.properties.ZCTA10) AND isnotnull(features#10292.properties.COMM))\n",
      "                                                +- Generate explode(features#10284), false, [features#10292]\n",
      "                                                   +- Filter ((size(features#10284, true) > 0) AND isnotnull(features#10284))\n",
      "                                                      +- FileScan geojson [features#10284] Batched: false, DataFilters: [(size(features#10284, true) > 0), isnotnull(features#10284)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "\n",
      "Broadcast Join:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#4107, Total Crimes#4602L, Total Population#4604L, Crime Per Capita#4608, Mean Estimated Income#10679]\n",
      "   +- BroadcastHashJoin [COMM#4107], [COMM#10308], Inner, BuildRight, false\n",
      "      :- Project [COMM#4107, Total Crimes#4602L, Total Population#4604L, (cast(Total Crimes#4602L as double) / cast(Total Population#4604L as double)) AS Crime Per Capita#4608]\n",
      "      :  +- HashAggregate(keys=[COMM#4107], functions=[count(1), sum(Total Population#4313L)], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#4107, 1000), ENSURE_REQUIREMENTS, [plan_id=5099]\n",
      "      :        +- HashAggregate(keys=[COMM#4107], functions=[partial_count(1), partial_sum(Total Population#4313L)], schema specialized)\n",
      "      :           +- Project [COMM#4107, Total Population#4313L]\n",
      "      :              +- RangeJoin geom#4462: geometry, geometry#4320: geometry, WITHIN\n",
      "      :                 :- Union\n",
      "      :                 :  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#4462]\n",
      "      :                 :  :  +- Filter ((((isnotnull(LAT#55) AND isnotnull(LON#56)) AND NOT (LAT#55 = 0.0)) AND NOT (LON#56 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                 :  :     +- FileScan csv [LAT#55,LON#56] Batched: false, DataFilters: [isnotnull(LAT#55), isnotnull(LON#56), NOT (LAT#55 = 0.0), NOT (LON#56 = 0.0), isnotnull( **org.a..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "      :                 :  +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#10893]\n",
      "      :                 :     +- Filter ((((isnotnull(LAT#129) AND isnotnull(LON#130)) AND NOT (LAT#129 = 0.0)) AND NOT (LON#130 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                 :        +- FileScan csv [LAT#129,LON#130] Batched: false, DataFilters: [isnotnull(LAT#129), isnotnull(LON#130), NOT (LAT#129 = 0.0), NOT (LON#130 = 0.0), isnotnull( **o..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "      :                 +- Project [COMM#4107, Total Population#4313L, geometry#4320]\n",
      "      :                    +- Filter ((((isnotnull(Total Population#4313L) AND isnotnull(Total Housing Units#4315L)) AND (Total Population#4313L > 0)) AND (Total Housing Units#4315L > 0)) AND isnotnull(geometry#4320))\n",
      "      :                       +- ObjectHashAggregate(keys=[COMM#4107, ZCTA10#4124], functions=[sum(POP_2010#4116L), sum(HOUSING10#4113L), st_union_aggr(geometry#4094, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35967ac4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "      :                          +- Exchange hashpartitioning(COMM#4107, ZCTA10#4124, 1000), ENSURE_REQUIREMENTS, [plan_id=5091]\n",
      "      :                             +- ObjectHashAggregate(keys=[COMM#4107, ZCTA10#4124], functions=[partial_sum(POP_2010#4116L), partial_sum(HOUSING10#4113L), partial_st_union_aggr(geometry#4094, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35967ac4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "      :                                +- Project [features#4091.properties.COMM AS COMM#4107, features#4091.properties.HOUSING10 AS HOUSING10#4113L, features#4091.properties.POP_2010 AS POP_2010#4116L, features#4091.properties.ZCTA10 AS ZCTA10#4124, features#4091.geometry AS geometry#4094]\n",
      "      :                                   +- Filter isnotnull(features#4091.properties.COMM)\n",
      "      :                                      +- Generate explode(features#4083), false, [features#4091]\n",
      "      :                                         +- Filter ((size(features#4083, true) > 0) AND isnotnull(features#4083))\n",
      "      :                                            +- FileScan geojson [features#4083] Batched: false, DataFilters: [(size(features#4083, true) > 0), isnotnull(features#4083)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=5115]\n",
      "         +- HashAggregate(keys=[COMM#10308], functions=[avg(Estimated Median Income#10453)], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#10308, 1000), ENSURE_REQUIREMENTS, [plan_id=5112]\n",
      "               +- HashAggregate(keys=[COMM#10308], functions=[partial_avg(Estimated Median Income#10453)], schema specialized)\n",
      "                  +- Project [Estimated Median Income#10453, COMM#10308]\n",
      "                     +- BroadcastHashJoin [Zip Code#10447], [cast(ZCTA10#10325 as int)], Inner, BuildRight, false\n",
      "                        :- Project [Zip Code#10447, cast(regexp_replace(Estimated Median Income#10449, [^0-9.], , 1) as double) AS Estimated Median Income#10453]\n",
      "                        :  +- Filter isnotnull(Zip Code#10447)\n",
      "                        :     +- FileScan csv [Zip Code#10447,Estimated Median Income#10449] Batched: false, DataFilters: [isnotnull(Zip Code#10447)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "                        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[1, string, true] as int) as bigint)),false), [plan_id=5107]\n",
      "                           +- Project [COMM#10308, ZCTA10#10325]\n",
      "                              +- Filter (((isnotnull(Total Population#10514L) AND isnotnull(Total Housing Units#10516L)) AND (Total Population#10514L > 0)) AND (Total Housing Units#10516L > 0))\n",
      "                                 +- HashAggregate(keys=[COMM#10308, ZCTA10#10325], functions=[sum(POP_2010#10317L), sum(HOUSING10#10314L)], schema specialized)\n",
      "                                    +- Exchange hashpartitioning(COMM#10308, ZCTA10#10325, 1000), ENSURE_REQUIREMENTS, [plan_id=5102]\n",
      "                                       +- HashAggregate(keys=[COMM#10308, ZCTA10#10325], functions=[partial_sum(POP_2010#10317L), partial_sum(HOUSING10#10314L)], schema specialized)\n",
      "                                          +- Project [features#10292.properties.COMM AS COMM#10308, features#10292.properties.HOUSING10 AS HOUSING10#10314L, features#10292.properties.POP_2010 AS POP_2010#10317L, features#10292.properties.ZCTA10 AS ZCTA10#10325]\n",
      "                                             +- Filter (isnotnull(features#10292.properties.ZCTA10) AND isnotnull(features#10292.properties.COMM))\n",
      "                                                +- Generate explode(features#10284), false, [features#10292]\n",
      "                                                   +- Filter ((size(features#10284, true) > 0) AND isnotnull(features#10284))\n",
      "                                                      +- FileScan geojson [features#10284] Batched: false, DataFilters: [(size(features#10284, true) > 0), isnotnull(features#10284)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "\n",
      "Merge Join:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#4107, Total Crimes#4602L, Total Population#4604L, Crime Per Capita#4608, Mean Estimated Income#10679]\n",
      "   +- SortMergeJoin [COMM#4107], [COMM#10308], Inner\n",
      "      :- Sort [COMM#4107 ASC NULLS FIRST], false, 0\n",
      "      :  +- Project [COMM#4107, Total Crimes#4602L, Total Population#4604L, (cast(Total Crimes#4602L as double) / cast(Total Population#4604L as double)) AS Crime Per Capita#4608]\n",
      "      :     +- HashAggregate(keys=[COMM#4107], functions=[count(1), sum(Total Population#4313L)], schema specialized)\n",
      "      :        +- Exchange hashpartitioning(COMM#4107, 1000), ENSURE_REQUIREMENTS, [plan_id=5514]\n",
      "      :           +- HashAggregate(keys=[COMM#4107], functions=[partial_count(1), partial_sum(Total Population#4313L)], schema specialized)\n",
      "      :              +- Project [COMM#4107, Total Population#4313L]\n",
      "      :                 +- RangeJoin geom#4462: geometry, geometry#4320: geometry, WITHIN\n",
      "      :                    :- Union\n",
      "      :                    :  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#4462]\n",
      "      :                    :  :  +- Filter ((((isnotnull(LAT#55) AND isnotnull(LON#56)) AND NOT (LAT#55 = 0.0)) AND NOT (LON#56 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                    :  :     +- FileScan csv [LAT#55,LON#56] Batched: false, DataFilters: [isnotnull(LAT#55), isnotnull(LON#56), NOT (LAT#55 = 0.0), NOT (LON#56 = 0.0), isnotnull( **org.a..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "      :                    :  +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#10926]\n",
      "      :                    :     +- Filter ((((isnotnull(LAT#129) AND isnotnull(LON#130)) AND NOT (LAT#129 = 0.0)) AND NOT (LON#130 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                    :        +- FileScan csv [LAT#129,LON#130] Batched: false, DataFilters: [isnotnull(LAT#129), isnotnull(LON#130), NOT (LAT#129 = 0.0), NOT (LON#130 = 0.0), isnotnull( **o..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "      :                    +- Project [COMM#4107, Total Population#4313L, geometry#4320]\n",
      "      :                       +- Filter ((((isnotnull(Total Population#4313L) AND isnotnull(Total Housing Units#4315L)) AND (Total Population#4313L > 0)) AND (Total Housing Units#4315L > 0)) AND isnotnull(geometry#4320))\n",
      "      :                          +- ObjectHashAggregate(keys=[COMM#4107, ZCTA10#4124], functions=[sum(POP_2010#4116L), sum(HOUSING10#4113L), st_union_aggr(geometry#4094, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35967ac4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "      :                             +- Exchange hashpartitioning(COMM#4107, ZCTA10#4124, 1000), ENSURE_REQUIREMENTS, [plan_id=5507]\n",
      "      :                                +- ObjectHashAggregate(keys=[COMM#4107, ZCTA10#4124], functions=[partial_sum(POP_2010#4116L), partial_sum(HOUSING10#4113L), partial_st_union_aggr(geometry#4094, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35967ac4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "      :                                   +- Project [features#4091.properties.COMM AS COMM#4107, features#4091.properties.HOUSING10 AS HOUSING10#4113L, features#4091.properties.POP_2010 AS POP_2010#4116L, features#4091.properties.ZCTA10 AS ZCTA10#4124, features#4091.geometry AS geometry#4094]\n",
      "      :                                      +- Filter isnotnull(features#4091.properties.COMM)\n",
      "      :                                         +- Generate explode(features#4083), false, [features#4091]\n",
      "      :                                            +- Filter ((size(features#4083, true) > 0) AND isnotnull(features#4083))\n",
      "      :                                               +- FileScan geojson [features#4083] Batched: false, DataFilters: [(size(features#4083, true) > 0), isnotnull(features#4083)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- Sort [COMM#10308 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#10308], functions=[avg(Estimated Median Income#10453)], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#10308, 1000), ENSURE_REQUIREMENTS, [plan_id=5480]\n",
      "               +- HashAggregate(keys=[COMM#10308], functions=[partial_avg(Estimated Median Income#10453)], schema specialized)\n",
      "                  +- Project [Estimated Median Income#10453, COMM#10308]\n",
      "                     +- BroadcastHashJoin [Zip Code#10447], [cast(ZCTA10#10325 as int)], Inner, BuildRight, false\n",
      "                        :- Project [Zip Code#10447, cast(regexp_replace(Estimated Median Income#10449, [^0-9.], , 1) as double) AS Estimated Median Income#10453]\n",
      "                        :  +- Filter isnotnull(Zip Code#10447)\n",
      "                        :     +- FileScan csv [Zip Code#10447,Estimated Median Income#10449] Batched: false, DataFilters: [isnotnull(Zip Code#10447)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "                        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[1, string, true] as int) as bigint)),false), [plan_id=5475]\n",
      "                           +- Project [COMM#10308, ZCTA10#10325]\n",
      "                              +- Filter (((isnotnull(Total Population#10514L) AND isnotnull(Total Housing Units#10516L)) AND (Total Population#10514L > 0)) AND (Total Housing Units#10516L > 0))\n",
      "                                 +- HashAggregate(keys=[COMM#10308, ZCTA10#10325], functions=[sum(POP_2010#10317L), sum(HOUSING10#10314L)], schema specialized)\n",
      "                                    +- Exchange hashpartitioning(COMM#10308, ZCTA10#10325, 1000), ENSURE_REQUIREMENTS, [plan_id=5470]\n",
      "                                       +- HashAggregate(keys=[COMM#10308, ZCTA10#10325], functions=[partial_sum(POP_2010#10317L), partial_sum(HOUSING10#10314L)], schema specialized)\n",
      "                                          +- Project [features#10292.properties.COMM AS COMM#10308, features#10292.properties.HOUSING10 AS HOUSING10#10314L, features#10292.properties.POP_2010 AS POP_2010#10317L, features#10292.properties.ZCTA10 AS ZCTA10#10325]\n",
      "                                             +- Filter (isnotnull(features#10292.properties.ZCTA10) AND isnotnull(features#10292.properties.COMM))\n",
      "                                                +- Generate explode(features#10284), false, [features#10292]\n",
      "                                                   +- Filter ((size(features#10284, true) > 0) AND isnotnull(features#10284))\n",
      "                                                      +- FileScan geojson [features#10284] Batched: false, DataFilters: [(size(features#10284, true) > 0), isnotnull(features#10284)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "\n",
      "Shuffle Hash Join:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#4107, Total Crimes#4602L, Total Population#4604L, Crime Per Capita#4608, Mean Estimated Income#10679]\n",
      "   +- ShuffledHashJoin [COMM#4107], [COMM#10308], Inner, BuildRight\n",
      "      :- Project [COMM#4107, Total Crimes#4602L, Total Population#4604L, (cast(Total Crimes#4602L as double) / cast(Total Population#4604L as double)) AS Crime Per Capita#4608]\n",
      "      :  +- HashAggregate(keys=[COMM#4107], functions=[count(1), sum(Total Population#4313L)], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#4107, 1000), ENSURE_REQUIREMENTS, [plan_id=5915]\n",
      "      :        +- HashAggregate(keys=[COMM#4107], functions=[partial_count(1), partial_sum(Total Population#4313L)], schema specialized)\n",
      "      :           +- Project [COMM#4107, Total Population#4313L]\n",
      "      :              +- RangeJoin geom#4462: geometry, geometry#4320: geometry, WITHIN\n",
      "      :                 :- Union\n",
      "      :                 :  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#4462]\n",
      "      :                 :  :  +- Filter ((((isnotnull(LAT#55) AND isnotnull(LON#56)) AND NOT (LAT#55 = 0.0)) AND NOT (LON#56 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                 :  :     +- FileScan csv [LAT#55,LON#56] Batched: false, DataFilters: [isnotnull(LAT#55), isnotnull(LON#56), NOT (LAT#55 = 0.0), NOT (LON#56 = 0.0), isnotnull( **org.a..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "      :                 :  +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#10960]\n",
      "      :                 :     +- Filter ((((isnotnull(LAT#129) AND isnotnull(LON#130)) AND NOT (LAT#129 = 0.0)) AND NOT (LON#130 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                 :        +- FileScan csv [LAT#129,LON#130] Batched: false, DataFilters: [isnotnull(LAT#129), isnotnull(LON#130), NOT (LAT#129 = 0.0), NOT (LON#130 = 0.0), isnotnull( **o..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "      :                 +- Project [COMM#4107, Total Population#4313L, geometry#4320]\n",
      "      :                    +- Filter ((((isnotnull(Total Population#4313L) AND isnotnull(Total Housing Units#4315L)) AND (Total Population#4313L > 0)) AND (Total Housing Units#4315L > 0)) AND isnotnull(geometry#4320))\n",
      "      :                       +- ObjectHashAggregate(keys=[COMM#4107, ZCTA10#4124], functions=[sum(POP_2010#4116L), sum(HOUSING10#4113L), st_union_aggr(geometry#4094, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35967ac4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "      :                          +- Exchange hashpartitioning(COMM#4107, ZCTA10#4124, 1000), ENSURE_REQUIREMENTS, [plan_id=5908]\n",
      "      :                             +- ObjectHashAggregate(keys=[COMM#4107, ZCTA10#4124], functions=[partial_sum(POP_2010#4116L), partial_sum(HOUSING10#4113L), partial_st_union_aggr(geometry#4094, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35967ac4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "      :                                +- Project [features#4091.properties.COMM AS COMM#4107, features#4091.properties.HOUSING10 AS HOUSING10#4113L, features#4091.properties.POP_2010 AS POP_2010#4116L, features#4091.properties.ZCTA10 AS ZCTA10#4124, features#4091.geometry AS geometry#4094]\n",
      "      :                                   +- Filter isnotnull(features#4091.properties.COMM)\n",
      "      :                                      +- Generate explode(features#4083), false, [features#4091]\n",
      "      :                                         +- Filter ((size(features#4083, true) > 0) AND isnotnull(features#4083))\n",
      "      :                                            +- FileScan geojson [features#4083] Batched: false, DataFilters: [(size(features#4083, true) > 0), isnotnull(features#4083)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- HashAggregate(keys=[COMM#10308], functions=[avg(Estimated Median Income#10453)], schema specialized)\n",
      "         +- Exchange hashpartitioning(COMM#10308, 1000), ENSURE_REQUIREMENTS, [plan_id=5885]\n",
      "            +- HashAggregate(keys=[COMM#10308], functions=[partial_avg(Estimated Median Income#10453)], schema specialized)\n",
      "               +- Project [Estimated Median Income#10453, COMM#10308]\n",
      "                  +- BroadcastHashJoin [Zip Code#10447], [cast(ZCTA10#10325 as int)], Inner, BuildRight, false\n",
      "                     :- Project [Zip Code#10447, cast(regexp_replace(Estimated Median Income#10449, [^0-9.], , 1) as double) AS Estimated Median Income#10453]\n",
      "                     :  +- Filter isnotnull(Zip Code#10447)\n",
      "                     :     +- FileScan csv [Zip Code#10447,Estimated Median Income#10449] Batched: false, DataFilters: [isnotnull(Zip Code#10447)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[1, string, true] as int) as bigint)),false), [plan_id=5880]\n",
      "                        +- Project [COMM#10308, ZCTA10#10325]\n",
      "                           +- Filter (((isnotnull(Total Population#10514L) AND isnotnull(Total Housing Units#10516L)) AND (Total Population#10514L > 0)) AND (Total Housing Units#10516L > 0))\n",
      "                              +- HashAggregate(keys=[COMM#10308, ZCTA10#10325], functions=[sum(POP_2010#10317L), sum(HOUSING10#10314L)], schema specialized)\n",
      "                                 +- Exchange hashpartitioning(COMM#10308, ZCTA10#10325, 1000), ENSURE_REQUIREMENTS, [plan_id=5875]\n",
      "                                    +- HashAggregate(keys=[COMM#10308, ZCTA10#10325], functions=[partial_sum(POP_2010#10317L), partial_sum(HOUSING10#10314L)], schema specialized)\n",
      "                                       +- Project [features#10292.properties.COMM AS COMM#10308, features#10292.properties.HOUSING10 AS HOUSING10#10314L, features#10292.properties.POP_2010 AS POP_2010#10317L, features#10292.properties.ZCTA10 AS ZCTA10#10325]\n",
      "                                          +- Filter (isnotnull(features#10292.properties.ZCTA10) AND isnotnull(features#10292.properties.COMM))\n",
      "                                             +- Generate explode(features#10284), false, [features#10292]\n",
      "                                                +- Filter ((size(features#10284, true) > 0) AND isnotnull(features#10284))\n",
      "                                                   +- FileScan geojson [features#10284] Batched: false, DataFilters: [(size(features#10284, true) > 0), isnotnull(features#10284)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "\n",
      "Shuffle Replicate NL Join:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#4107, Total Crimes#4602L, Total Population#4604L, Crime Per Capita#4608, Mean Estimated Income#10679]\n",
      "   +- CartesianProduct (COMM#4107 = COMM#10308)\n",
      "      :- Project [COMM#4107, Total Crimes#4602L, Total Population#4604L, (cast(Total Crimes#4602L as double) / cast(Total Population#4604L as double)) AS Crime Per Capita#4608]\n",
      "      :  +- HashAggregate(keys=[COMM#4107], functions=[count(1), sum(Total Population#4313L)], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#4107, 1000), ENSURE_REQUIREMENTS, [plan_id=6310]\n",
      "      :        +- HashAggregate(keys=[COMM#4107], functions=[partial_count(1), partial_sum(Total Population#4313L)], schema specialized)\n",
      "      :           +- Project [COMM#4107, Total Population#4313L]\n",
      "      :              +- RangeJoin geom#4462: geometry, geometry#4320: geometry, WITHIN\n",
      "      :                 :- Union\n",
      "      :                 :  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#4462]\n",
      "      :                 :  :  +- Filter ((((isnotnull(LAT#55) AND isnotnull(LON#56)) AND NOT (LAT#55 = 0.0)) AND NOT (LON#56 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                 :  :     +- FileScan csv [LAT#55,LON#56] Batched: false, DataFilters: [isnotnull(LAT#55), isnotnull(LON#56), NOT (LAT#55 = 0.0), NOT (LON#56 = 0.0), isnotnull( **org.a..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "      :                 :  +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#10994]\n",
      "      :                 :     +- Filter ((((isnotnull(LAT#129) AND isnotnull(LON#130)) AND NOT (LAT#129 = 0.0)) AND NOT (LON#130 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                 :        +- FileScan csv [LAT#129,LON#130] Batched: false, DataFilters: [isnotnull(LAT#129), isnotnull(LON#130), NOT (LAT#129 = 0.0), NOT (LON#130 = 0.0), isnotnull( **o..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<LAT:double,LON:double>\n",
      "      :                 +- Project [COMM#4107, Total Population#4313L, geometry#4320]\n",
      "      :                    +- Filter ((((isnotnull(Total Population#4313L) AND isnotnull(Total Housing Units#4315L)) AND (Total Population#4313L > 0)) AND (Total Housing Units#4315L > 0)) AND isnotnull(geometry#4320))\n",
      "      :                       +- ObjectHashAggregate(keys=[COMM#4107, ZCTA10#4124], functions=[sum(POP_2010#4116L), sum(HOUSING10#4113L), st_union_aggr(geometry#4094, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35967ac4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "      :                          +- Exchange hashpartitioning(COMM#4107, ZCTA10#4124, 1000), ENSURE_REQUIREMENTS, [plan_id=6303]\n",
      "      :                             +- ObjectHashAggregate(keys=[COMM#4107, ZCTA10#4124], functions=[partial_sum(POP_2010#4116L), partial_sum(HOUSING10#4113L), partial_st_union_aggr(geometry#4094, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35967ac4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "      :                                +- Project [features#4091.properties.COMM AS COMM#4107, features#4091.properties.HOUSING10 AS HOUSING10#4113L, features#4091.properties.POP_2010 AS POP_2010#4116L, features#4091.properties.ZCTA10 AS ZCTA10#4124, features#4091.geometry AS geometry#4094]\n",
      "      :                                   +- Filter isnotnull(features#4091.properties.COMM)\n",
      "      :                                      +- Generate explode(features#4083), false, [features#4091]\n",
      "      :                                         +- Filter ((size(features#4083, true) > 0) AND isnotnull(features#4083))\n",
      "      :                                            +- FileScan geojson [features#4083] Batched: false, DataFilters: [(size(features#4083, true) > 0), isnotnull(features#4083)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- HashAggregate(keys=[COMM#10308], functions=[avg(Estimated Median Income#10453)], schema specialized)\n",
      "         +- Exchange hashpartitioning(COMM#10308, 1000), ENSURE_REQUIREMENTS, [plan_id=6281]\n",
      "            +- HashAggregate(keys=[COMM#10308], functions=[partial_avg(Estimated Median Income#10453)], schema specialized)\n",
      "               +- Project [Estimated Median Income#10453, COMM#10308]\n",
      "                  +- BroadcastHashJoin [Zip Code#10447], [cast(ZCTA10#10325 as int)], Inner, BuildRight, false\n",
      "                     :- Project [Zip Code#10447, cast(regexp_replace(Estimated Median Income#10449, [^0-9.], , 1) as double) AS Estimated Median Income#10453]\n",
      "                     :  +- Filter isnotnull(Zip Code#10447)\n",
      "                     :     +- FileScan csv [Zip Code#10447,Estimated Median Income#10449] Batched: false, DataFilters: [isnotnull(Zip Code#10447)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[1, string, true] as int) as bigint)),false), [plan_id=6276]\n",
      "                        +- Project [COMM#10308, ZCTA10#10325]\n",
      "                           +- Filter (((isnotnull(Total Population#10514L) AND isnotnull(Total Housing Units#10516L)) AND (Total Population#10514L > 0)) AND (Total Housing Units#10516L > 0))\n",
      "                              +- HashAggregate(keys=[COMM#10308, ZCTA10#10325], functions=[sum(POP_2010#10317L), sum(HOUSING10#10314L)], schema specialized)\n",
      "                                 +- Exchange hashpartitioning(COMM#10308, ZCTA10#10325, 1000), ENSURE_REQUIREMENTS, [plan_id=6271]\n",
      "                                    +- HashAggregate(keys=[COMM#10308, ZCTA10#10325], functions=[partial_sum(POP_2010#10317L), partial_sum(HOUSING10#10314L)], schema specialized)\n",
      "                                       +- Project [features#10292.properties.COMM AS COMM#10308, features#10292.properties.HOUSING10 AS HOUSING10#10314L, features#10292.properties.POP_2010 AS POP_2010#10317L, features#10292.properties.ZCTA10 AS ZCTA10#10325]\n",
      "                                          +- Filter (isnotnull(features#10292.properties.ZCTA10) AND isnotnull(features#10292.properties.COMM))\n",
      "                                             +- Generate explode(features#10284), false, [features#10292]\n",
      "                                                +- Filter ((size(features#10284, true) > 0) AND isnotnull(features#10284))\n",
      "                                                   +- FileScan geojson [features#10284] Batched: false, DataFilters: [(size(features#10284, true) > 0), isnotnull(features#10284)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:..."
     ]
    }
   ],
   "source": [
    "#JOINS\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Ενδεικτικό Join χωρίς hint\n",
    "basic_join = crime_population_df.join(\n",
    "    income_aggregated_df,\n",
    "    \"COMM\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Εμφάνιση του φυσικού πλάνου εκτέλεσης\n",
    "print(\"Basic Join:\")\n",
    "basic_join.explain()\n",
    "\n",
    "# Προσθήκη Broadcast Hint\n",
    "broadcast_join = crime_population_df.join(\n",
    "    broadcast(income_aggregated_df),\n",
    "    \"COMM\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\nBroadcast Join:\")\n",
    "broadcast_join.explain()\n",
    "\n",
    "# Προσθήκη Merge Hint\n",
    "merge_join = crime_population_df.hint(\"merge\").join(\n",
    "    income_aggregated_df.hint(\"merge\"),\n",
    "    \"COMM\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\nMerge Join:\")\n",
    "merge_join.explain()\n",
    "\n",
    "# Προσθήκη Shuffle Hash Hint\n",
    "shuffle_hash_join = crime_population_df.hint(\"shuffle_hash\").join(\n",
    "    income_aggregated_df.hint(\"shuffle_hash\"),\n",
    "    \"COMM\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\nShuffle Hash Join:\")\n",
    "shuffle_hash_join.explain()\n",
    "\n",
    "# Προσθήκη Shuffle Replicate NL Hint\n",
    "shuffle_replicate_nl_join = crime_population_df.hint(\"shuffle_replicate_nl\").join(\n",
    "    income_aggregated_df.hint(\"shuffle_replicate_nl\"),\n",
    "    \"COMM\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\nShuffle Replicate NL Join:\")\n",
    "shuffle_replicate_nl_join.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbfa99cd-1ac1-4234-b795-5f5d4320617d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "unexpected indent (<stdin>, line 76)\n",
      "  File \"<stdin>\", line 76\n",
      "    print(f\"Execution Time: {start_time - end_time} seconds\\n\")\n",
      "IndentationError: unexpected indent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUERY5\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Συνάρτηση για τη δημιουργία νέου SparkSession\n",
    "def create_spark_session(executors, cores_per_executor, memory_per_executor):\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Query5 Execution\") \\\n",
    "        .config(\"spark.executor.instances\", executors) \\\n",
    "        .config(\"spark.executor.cores\", cores_per_executor) \\\n",
    "        .config(\"spark.executor.memory\", f\"{memory_per_executor}G\") \\\n",
    "        .config(\"spark.driver.memory\", \"4G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Διαφορετικά configurations\n",
    "configs = [\n",
    "    (2, 4, 8),  # 2 executors × 4 cores/8GB memory\n",
    "    (4, 2, 4),  # 4 executors × 2 cores/4GB memory\n",
    "    (8, 1, 2)   # 8 executors × 1 core/2GB memory\n",
    "]\n",
    "\n",
    "# Επανάληψη για κάθε configuration\n",
    "for executors, cores, memory in configs:\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "\n",
    "# Δημιουργία νέου SparkSession\n",
    "    spark = create_spark_session(executors, cores, memory)\n",
    "    print(f\"Executing Query 5 with {executors} executors × {cores} cores/{memory}GB memory\")\n",
    "\n",
    "\n",
    "police_stations_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "\n",
    "# Φόρτωση δεδομένων αστυνομικών τμημάτων\n",
    "police_stations_df = spark.read.csv(police_stations_path, header=True, inferSchema=True)\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "\n",
    "# Εγγραφή Sedona functions\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Μετατροπή δεδομένων εγκλημάτων σε γεωμετρικά σημεία\n",
    "filtered_crime_df = filtered_crime_df.withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "# Μετατροπή δεδομένων αστυνομικών τμημάτων σε γεωμετρικά σημεία\n",
    "police_stations_df = police_stations_df.withColumn(\"geom\", ST_Point(\"X\", \"Y\"))\n",
    "\n",
    "# Υπολογισμός απόστασης μεταξύ σημείων εγκλημάτων και αστυνομικών τμημάτων\n",
    "crime_with_distance = filtered_crime_df.crossJoin(police_stations_df).withColumn(\n",
    "    \"distance\",\n",
    "    ST_DistanceSphere(filtered_crime_df[\"geom\"], police_stations_df[\"geom\"]) / 1000  # Απόσταση σε χιλιόμετρα\n",
    ")\n",
    "\n",
    "# Εύρεση του πλησιέστερου τμήματος για κάθε έγκλημα\n",
    "window_spec = Window.partitionBy(\"DR_NO\").orderBy(\"distance\")\n",
    "crime_with_nearest_station = crime_with_distance.withColumn(\n",
    "    \"row_num\",\n",
    "    row_number().over(window_spec)\n",
    ").filter(\"row_num = 1\").drop(\"row_num\")\n",
    "\n",
    "# Ομαδοποίηση ανά αστυνομικό τμήμα\n",
    "result_3_df = crime_with_nearest_station.groupBy(\"DIVISION\").agg(\n",
    "    count(\"*\").alias(\"Total Crimes\"),\n",
    "    avg(\"distance\").alias(\"Average Distance\")\n",
    ").orderBy(\"Total Crimes\", ascending=False)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Εμφάνιση των αποτελεσμάτων\n",
    "result_3_df.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "    print(f\"Execution Time: {start_time - end_time} seconds\\n\")\n",
    "\n",
    "# Τερματισμός της συνεδρίας για να επιτρέψει την αλλαγή των ρυθμίσεων\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d98e1-900f-4d64-bc26-21fc02a1c2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
